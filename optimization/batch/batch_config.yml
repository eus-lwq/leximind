model: "meta-llama/Llama-2-13b-chat-hf"
gpu_params:
  tensor_parallel_size: 1
  dtype: float16
  max_seq_len: 4096
  block_size: 16
  swap_space: 4  # GB

batch_sizes: [4, 8, 16, 32, 64]  # Test values for GPU

test_params:
  num_requests: 1000
  request_rate: 10    # Requests/second
  max_tokens: 256
  temperature: 0.7