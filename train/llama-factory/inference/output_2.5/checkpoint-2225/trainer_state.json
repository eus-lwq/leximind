{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.4974035087719297,
  "eval_steps": 500,
  "global_step": 2225,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.02245614035087719,
      "grad_norm": 10.320698738098145,
      "learning_rate": 8.520179372197309e-07,
      "loss": 7.6902,
      "step": 20
    },
    {
      "epoch": 0.04491228070175438,
      "grad_norm": 12.566347122192383,
      "learning_rate": 1.7488789237668164e-06,
      "loss": 7.6805,
      "step": 40
    },
    {
      "epoch": 0.06736842105263158,
      "grad_norm": 13.478910446166992,
      "learning_rate": 2.6457399103139016e-06,
      "loss": 7.5908,
      "step": 60
    },
    {
      "epoch": 0.08982456140350877,
      "grad_norm": 11.578929901123047,
      "learning_rate": 3.542600896860987e-06,
      "loss": 7.2757,
      "step": 80
    },
    {
      "epoch": 0.11228070175438597,
      "grad_norm": 18.189550399780273,
      "learning_rate": 4.439461883408072e-06,
      "loss": 7.0303,
      "step": 100
    },
    {
      "epoch": 0.13473684210526315,
      "grad_norm": 18.684097290039062,
      "learning_rate": 5.3363228699551574e-06,
      "loss": 6.4911,
      "step": 120
    },
    {
      "epoch": 0.15719298245614036,
      "grad_norm": 20.687345504760742,
      "learning_rate": 6.233183856502243e-06,
      "loss": 5.5936,
      "step": 140
    },
    {
      "epoch": 0.17964912280701753,
      "grad_norm": 19.97447395324707,
      "learning_rate": 7.1300448430493275e-06,
      "loss": 4.2433,
      "step": 160
    },
    {
      "epoch": 0.20210526315789473,
      "grad_norm": 4.259260654449463,
      "learning_rate": 8.026905829596413e-06,
      "loss": 3.2481,
      "step": 180
    },
    {
      "epoch": 0.22456140350877193,
      "grad_norm": 3.770267963409424,
      "learning_rate": 8.923766816143498e-06,
      "loss": 2.6821,
      "step": 200
    },
    {
      "epoch": 0.24701754385964914,
      "grad_norm": 2.6076178550720215,
      "learning_rate": 9.820627802690584e-06,
      "loss": 2.4161,
      "step": 220
    },
    {
      "epoch": 0.2694736842105263,
      "grad_norm": 2.2574081420898438,
      "learning_rate": 9.998424099627158e-06,
      "loss": 2.3145,
      "step": 240
    },
    {
      "epoch": 0.2919298245614035,
      "grad_norm": 2.274693727493286,
      "learning_rate": 9.992023706854076e-06,
      "loss": 2.15,
      "step": 260
    },
    {
      "epoch": 0.3143859649122807,
      "grad_norm": 2.195364236831665,
      "learning_rate": 9.980706626858607e-06,
      "loss": 2.1305,
      "step": 280
    },
    {
      "epoch": 0.3368421052631579,
      "grad_norm": 2.531327962875366,
      "learning_rate": 9.964484005930486e-06,
      "loss": 2.1303,
      "step": 300
    },
    {
      "epoch": 0.35929824561403506,
      "grad_norm": 2.4338910579681396,
      "learning_rate": 9.943371821869044e-06,
      "loss": 2.1456,
      "step": 320
    },
    {
      "epoch": 0.3817543859649123,
      "grad_norm": 2.5686910152435303,
      "learning_rate": 9.917390868246529e-06,
      "loss": 2.1441,
      "step": 340
    },
    {
      "epoch": 0.40421052631578946,
      "grad_norm": 2.6378533840179443,
      "learning_rate": 9.886566733928336e-06,
      "loss": 2.0561,
      "step": 360
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 2.870979070663452,
      "learning_rate": 9.850929777870324e-06,
      "loss": 2.0345,
      "step": 380
    },
    {
      "epoch": 0.44912280701754387,
      "grad_norm": 3.010441780090332,
      "learning_rate": 9.810515099218004e-06,
      "loss": 1.9698,
      "step": 400
    },
    {
      "epoch": 0.47157894736842104,
      "grad_norm": 3.5716187953948975,
      "learning_rate": 9.765362502737098e-06,
      "loss": 1.9796,
      "step": 420
    },
    {
      "epoch": 0.49403508771929827,
      "grad_norm": 2.6153812408447266,
      "learning_rate": 9.715516459609478e-06,
      "loss": 1.9499,
      "step": 440
    },
    {
      "epoch": 0.5164912280701754,
      "grad_norm": 4.146578311920166,
      "learning_rate": 9.661026063633117e-06,
      "loss": 2.0417,
      "step": 460
    },
    {
      "epoch": 0.5389473684210526,
      "grad_norm": 3.2544853687286377,
      "learning_rate": 9.601944982869177e-06,
      "loss": 2.0188,
      "step": 480
    },
    {
      "epoch": 0.5614035087719298,
      "grad_norm": 3.6682071685791016,
      "learning_rate": 9.538331406783886e-06,
      "loss": 1.9556,
      "step": 500
    },
    {
      "epoch": 0.583859649122807,
      "grad_norm": 3.147333860397339,
      "learning_rate": 9.470247988937215e-06,
      "loss": 1.9444,
      "step": 520
    },
    {
      "epoch": 0.6063157894736843,
      "grad_norm": 3.511176824569702,
      "learning_rate": 9.397761785274855e-06,
      "loss": 1.8708,
      "step": 540
    },
    {
      "epoch": 0.6287719298245614,
      "grad_norm": 3.3314268589019775,
      "learning_rate": 9.320944188084241e-06,
      "loss": 1.907,
      "step": 560
    },
    {
      "epoch": 0.6512280701754386,
      "grad_norm": 3.9020113945007324,
      "learning_rate": 9.239870855679665e-06,
      "loss": 1.9135,
      "step": 580
    },
    {
      "epoch": 0.6736842105263158,
      "grad_norm": 3.530691623687744,
      "learning_rate": 9.154621637885745e-06,
      "loss": 1.8758,
      "step": 600
    },
    {
      "epoch": 0.696140350877193,
      "grad_norm": 4.529221057891846,
      "learning_rate": 9.065280497392663e-06,
      "loss": 1.9558,
      "step": 620
    },
    {
      "epoch": 0.7185964912280701,
      "grad_norm": 4.020026683807373,
      "learning_rate": 8.971935427060563e-06,
      "loss": 1.9303,
      "step": 640
    },
    {
      "epoch": 0.7410526315789474,
      "grad_norm": 3.5402517318725586,
      "learning_rate": 8.874678363254643e-06,
      "loss": 1.9303,
      "step": 660
    },
    {
      "epoch": 0.7635087719298246,
      "grad_norm": 4.267966270446777,
      "learning_rate": 8.773605095296223e-06,
      "loss": 1.8903,
      "step": 680
    },
    {
      "epoch": 0.7859649122807018,
      "grad_norm": 4.0449748039245605,
      "learning_rate": 8.66881517111902e-06,
      "loss": 1.8673,
      "step": 700
    },
    {
      "epoch": 0.8084210526315789,
      "grad_norm": 3.876464366912842,
      "learning_rate": 8.560411799223538e-06,
      "loss": 1.8982,
      "step": 720
    },
    {
      "epoch": 0.8308771929824561,
      "grad_norm": 4.198797225952148,
      "learning_rate": 8.448501747026108e-06,
      "loss": 1.9371,
      "step": 740
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 3.6550495624542236,
      "learning_rate": 8.333195235702752e-06,
      "loss": 1.9154,
      "step": 760
    },
    {
      "epoch": 0.8757894736842106,
      "grad_norm": 3.665475606918335,
      "learning_rate": 8.21460583163139e-06,
      "loss": 1.901,
      "step": 780
    },
    {
      "epoch": 0.8982456140350877,
      "grad_norm": 4.091419219970703,
      "learning_rate": 8.092850334539337e-06,
      "loss": 1.9202,
      "step": 800
    },
    {
      "epoch": 0.9207017543859649,
      "grad_norm": 4.039829730987549,
      "learning_rate": 7.968048662466244e-06,
      "loss": 1.8679,
      "step": 820
    },
    {
      "epoch": 0.9431578947368421,
      "grad_norm": 3.527869939804077,
      "learning_rate": 7.84032373365578e-06,
      "loss": 1.8293,
      "step": 840
    },
    {
      "epoch": 0.9656140350877193,
      "grad_norm": 3.818632125854492,
      "learning_rate": 7.709801345492403e-06,
      "loss": 1.8967,
      "step": 860
    },
    {
      "epoch": 0.9880701754385965,
      "grad_norm": 3.6736092567443848,
      "learning_rate": 7.576610050602419e-06,
      "loss": 1.8936,
      "step": 880
    },
    {
      "epoch": 1.0101052631578948,
      "grad_norm": 3.8322834968566895,
      "learning_rate": 7.440881030241407e-06,
      "loss": 1.8308,
      "step": 900
    },
    {
      "epoch": 1.032561403508772,
      "grad_norm": 4.3199663162231445,
      "learning_rate": 7.302747965092652e-06,
      "loss": 1.9233,
      "step": 920
    },
    {
      "epoch": 1.0550175438596492,
      "grad_norm": 4.005818843841553,
      "learning_rate": 7.1623469036038926e-06,
      "loss": 1.8121,
      "step": 940
    },
    {
      "epoch": 1.0774736842105264,
      "grad_norm": 3.7360777854919434,
      "learning_rate": 7.019816127992039e-06,
      "loss": 1.8322,
      "step": 960
    },
    {
      "epoch": 1.0999298245614035,
      "grad_norm": 3.9691100120544434,
      "learning_rate": 6.87529601804781e-06,
      "loss": 1.8458,
      "step": 980
    },
    {
      "epoch": 1.1223859649122807,
      "grad_norm": 5.5042548179626465,
      "learning_rate": 6.728928912874479e-06,
      "loss": 1.837,
      "step": 1000
    },
    {
      "epoch": 1.1448421052631579,
      "grad_norm": 3.74159574508667,
      "learning_rate": 6.5808589706968515e-06,
      "loss": 1.808,
      "step": 1020
    },
    {
      "epoch": 1.167298245614035,
      "grad_norm": 4.097579002380371,
      "learning_rate": 6.431232026878598e-06,
      "loss": 1.8779,
      "step": 1040
    },
    {
      "epoch": 1.1897543859649122,
      "grad_norm": 3.9392483234405518,
      "learning_rate": 6.280195450287736e-06,
      "loss": 1.8199,
      "step": 1060
    },
    {
      "epoch": 1.2122105263157894,
      "grad_norm": 3.9430181980133057,
      "learning_rate": 6.127897998151763e-06,
      "loss": 1.8115,
      "step": 1080
    },
    {
      "epoch": 1.2346666666666666,
      "grad_norm": 4.5021257400512695,
      "learning_rate": 5.974489669545379e-06,
      "loss": 1.7903,
      "step": 1100
    },
    {
      "epoch": 1.2571228070175438,
      "grad_norm": 4.16763162612915,
      "learning_rate": 5.820121557655109e-06,
      "loss": 1.805,
      "step": 1120
    },
    {
      "epoch": 1.279578947368421,
      "grad_norm": 13.564587593078613,
      "learning_rate": 5.6649457009663154e-06,
      "loss": 1.7793,
      "step": 1140
    },
    {
      "epoch": 1.3020350877192983,
      "grad_norm": 4.3138508796691895,
      "learning_rate": 5.509114933519179e-06,
      "loss": 1.8442,
      "step": 1160
    },
    {
      "epoch": 1.3244912280701755,
      "grad_norm": 5.205451488494873,
      "learning_rate": 5.352782734381137e-06,
      "loss": 1.8349,
      "step": 1180
    },
    {
      "epoch": 1.3469473684210527,
      "grad_norm": 3.9262638092041016,
      "learning_rate": 5.196103076484029e-06,
      "loss": 1.823,
      "step": 1200
    },
    {
      "epoch": 1.3694035087719298,
      "grad_norm": 3.8150925636291504,
      "learning_rate": 5.039230274974823e-06,
      "loss": 1.7739,
      "step": 1220
    },
    {
      "epoch": 1.391859649122807,
      "grad_norm": 3.7209272384643555,
      "learning_rate": 4.882318835229297e-06,
      "loss": 1.8359,
      "step": 1240
    },
    {
      "epoch": 1.4143157894736842,
      "grad_norm": 4.092719554901123,
      "learning_rate": 4.7255233006783626e-06,
      "loss": 1.8455,
      "step": 1260
    },
    {
      "epoch": 1.4367719298245614,
      "grad_norm": 4.243887424468994,
      "learning_rate": 4.568998100596903e-06,
      "loss": 1.7915,
      "step": 1280
    },
    {
      "epoch": 1.4592280701754385,
      "grad_norm": 5.0327935218811035,
      "learning_rate": 4.412897398005051e-06,
      "loss": 1.8123,
      "step": 1300
    },
    {
      "epoch": 1.4816842105263157,
      "grad_norm": 4.335853099822998,
      "learning_rate": 4.257374937831698e-06,
      "loss": 1.7339,
      "step": 1320
    },
    {
      "epoch": 1.504140350877193,
      "grad_norm": 4.548487186431885,
      "learning_rate": 4.10258389548979e-06,
      "loss": 1.7998,
      "step": 1340
    },
    {
      "epoch": 1.5265964912280703,
      "grad_norm": 4.055405616760254,
      "learning_rate": 3.948676726012538e-06,
      "loss": 1.844,
      "step": 1360
    },
    {
      "epoch": 1.5490526315789475,
      "grad_norm": 4.100946426391602,
      "learning_rate": 3.7958050138991437e-06,
      "loss": 1.8174,
      "step": 1380
    },
    {
      "epoch": 1.5715087719298246,
      "grad_norm": 4.265313148498535,
      "learning_rate": 3.6441193238179152e-06,
      "loss": 1.8754,
      "step": 1400
    },
    {
      "epoch": 1.5939649122807018,
      "grad_norm": 4.176671028137207,
      "learning_rate": 3.4937690523138302e-06,
      "loss": 1.8184,
      "step": 1420
    },
    {
      "epoch": 1.616421052631579,
      "grad_norm": 4.602894306182861,
      "learning_rate": 3.344902280666582e-06,
      "loss": 1.8109,
      "step": 1440
    },
    {
      "epoch": 1.6388771929824562,
      "grad_norm": 3.5732314586639404,
      "learning_rate": 3.19766562904406e-06,
      "loss": 1.8228,
      "step": 1460
    },
    {
      "epoch": 1.6613333333333333,
      "grad_norm": 4.146711349487305,
      "learning_rate": 3.052204112094873e-06,
      "loss": 1.8509,
      "step": 1480
    },
    {
      "epoch": 1.6837894736842105,
      "grad_norm": 4.29599666595459,
      "learning_rate": 2.9086609961221758e-06,
      "loss": 1.8693,
      "step": 1500
    },
    {
      "epoch": 1.7062456140350877,
      "grad_norm": 4.36772346496582,
      "learning_rate": 2.7671776579794442e-06,
      "loss": 1.7882,
      "step": 1520
    },
    {
      "epoch": 1.7287017543859649,
      "grad_norm": 4.245455741882324,
      "learning_rate": 2.6278934458271998e-06,
      "loss": 1.8498,
      "step": 1540
    },
    {
      "epoch": 1.751157894736842,
      "grad_norm": 4.362158298492432,
      "learning_rate": 2.4909455418877986e-06,
      "loss": 1.8415,
      "step": 1560
    },
    {
      "epoch": 1.7736140350877192,
      "grad_norm": 4.412906169891357,
      "learning_rate": 2.3564688273334857e-06,
      "loss": 1.757,
      "step": 1580
    },
    {
      "epoch": 1.7960701754385964,
      "grad_norm": 5.152838230133057,
      "learning_rate": 2.2245957494407527e-06,
      "loss": 1.852,
      "step": 1600
    },
    {
      "epoch": 1.8185263157894735,
      "grad_norm": 4.399362087249756,
      "learning_rate": 2.0954561911418743e-06,
      "loss": 1.8622,
      "step": 1620
    },
    {
      "epoch": 1.8409824561403507,
      "grad_norm": 4.290139675140381,
      "learning_rate": 1.9691773431020978e-06,
      "loss": 1.8445,
      "step": 1640
    },
    {
      "epoch": 1.8634385964912281,
      "grad_norm": 4.004946231842041,
      "learning_rate": 1.8458835784484503e-06,
      "loss": 1.8445,
      "step": 1660
    },
    {
      "epoch": 1.8858947368421053,
      "grad_norm": 4.223630905151367,
      "learning_rate": 1.7256963302735752e-06,
      "loss": 1.8036,
      "step": 1680
    },
    {
      "epoch": 1.9083508771929825,
      "grad_norm": 4.73323917388916,
      "learning_rate": 1.6087339720352308e-06,
      "loss": 1.8338,
      "step": 1700
    },
    {
      "epoch": 1.9308070175438596,
      "grad_norm": 3.9162588119506836,
      "learning_rate": 1.4951117009692528e-06,
      "loss": 1.7963,
      "step": 1720
    },
    {
      "epoch": 1.9532631578947368,
      "grad_norm": 4.30452299118042,
      "learning_rate": 1.3849414246307996e-06,
      "loss": 1.8048,
      "step": 1740
    },
    {
      "epoch": 1.975719298245614,
      "grad_norm": 4.87237024307251,
      "learning_rate": 1.2783316506756377e-06,
      "loss": 1.8245,
      "step": 1760
    },
    {
      "epoch": 1.9981754385964914,
      "grad_norm": 4.385042667388916,
      "learning_rate": 1.1753873799900134e-06,
      "loss": 1.7939,
      "step": 1780
    },
    {
      "epoch": 2.0202105263157897,
      "grad_norm": 4.094568729400635,
      "learning_rate": 1.0762100032743783e-06,
      "loss": 1.7862,
      "step": 1800
    },
    {
      "epoch": 2.042666666666667,
      "grad_norm": 5.158953666687012,
      "learning_rate": 9.808972011828055e-07,
      "loss": 1.7736,
      "step": 1820
    },
    {
      "epoch": 2.065122807017544,
      "grad_norm": 4.98980188369751,
      "learning_rate": 8.895428481164792e-07,
      "loss": 1.7851,
      "step": 1840
    },
    {
      "epoch": 2.087578947368421,
      "grad_norm": 4.968978404998779,
      "learning_rate": 8.022369197659824e-07,
      "loss": 1.775,
      "step": 1860
    },
    {
      "epoch": 2.1100350877192984,
      "grad_norm": 4.270637512207031,
      "learning_rate": 7.190654044934641e-07,
      "loss": 1.8245,
      "step": 1880
    },
    {
      "epoch": 2.1324912280701755,
      "grad_norm": 4.228272438049316,
      "learning_rate": 6.401102186419589e-07,
      "loss": 1.785,
      "step": 1900
    },
    {
      "epoch": 2.1549473684210527,
      "grad_norm": 4.850806713104248,
      "learning_rate": 5.654491258552735e-07,
      "loss": 1.8377,
      "step": 1920
    },
    {
      "epoch": 2.17740350877193,
      "grad_norm": 4.6046271324157715,
      "learning_rate": 4.951556604879049e-07,
      "loss": 1.802,
      "step": 1940
    },
    {
      "epoch": 2.199859649122807,
      "grad_norm": 4.441037178039551,
      "learning_rate": 4.2929905518041714e-07,
      "loss": 1.8027,
      "step": 1960
    },
    {
      "epoch": 2.2223157894736842,
      "grad_norm": 4.683783531188965,
      "learning_rate": 3.679441726716199e-07,
      "loss": 1.7691,
      "step": 1980
    },
    {
      "epoch": 2.2447719298245614,
      "grad_norm": 4.947292804718018,
      "learning_rate": 3.111514419146949e-07,
      "loss": 1.8353,
      "step": 2000
    },
    {
      "epoch": 2.2672280701754386,
      "grad_norm": 4.139573097229004,
      "learning_rate": 2.5897679856019767e-07,
      "loss": 1.7781,
      "step": 2020
    },
    {
      "epoch": 2.2896842105263158,
      "grad_norm": 4.511245250701904,
      "learning_rate": 2.1147162986455637e-07,
      "loss": 1.8387,
      "step": 2040
    },
    {
      "epoch": 2.312140350877193,
      "grad_norm": 5.419484615325928,
      "learning_rate": 1.6868272407831509e-07,
      "loss": 1.7974,
      "step": 2060
    },
    {
      "epoch": 2.33459649122807,
      "grad_norm": 5.407906532287598,
      "learning_rate": 1.3065222436398572e-07,
      "loss": 1.8028,
      "step": 2080
    },
    {
      "epoch": 2.3570526315789473,
      "grad_norm": 4.815282821655273,
      "learning_rate": 9.741758728888218e-08,
      "loss": 1.7851,
      "step": 2100
    },
    {
      "epoch": 2.3795087719298245,
      "grad_norm": 5.078648567199707,
      "learning_rate": 6.901154593382309e-08,
      "loss": 1.7812,
      "step": 2120
    },
    {
      "epoch": 2.4019649122807016,
      "grad_norm": 5.054312229156494,
      "learning_rate": 4.5462077654039874e-08,
      "loss": 1.8637,
      "step": 2140
    },
    {
      "epoch": 2.424421052631579,
      "grad_norm": 4.52987813949585,
      "learning_rate": 2.6792376524036878e-08,
      "loss": 1.7947,
      "step": 2160
    },
    {
      "epoch": 2.446877192982456,
      "grad_norm": 4.6973371505737305,
      "learning_rate": 1.302083049354752e-08,
      "loss": 1.8463,
      "step": 2180
    },
    {
      "epoch": 2.469333333333333,
      "grad_norm": 4.284024238586426,
      "learning_rate": 4.161003277085574e-09,
      "loss": 1.7803,
      "step": 2200
    },
    {
      "epoch": 2.4917894736842103,
      "grad_norm": 4.435617446899414,
      "learning_rate": 2.216209949229553e-10,
      "loss": 1.764,
      "step": 2220
    }
  ],
  "logging_steps": 20,
  "max_steps": 2225,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.0868628238130217e+18,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
