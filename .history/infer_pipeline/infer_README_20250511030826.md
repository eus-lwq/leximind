# LLaMA Inference Quick Guide

To perform inference using a LoRA-finetuned LLaMA model via `llama-factory` and `vLLM`, follow the steps below.

First, clone the repository:

```bash
git clone git@github.com:Yuan-33/llama-factory.git

Install all dependencies:

bash setup_infer.sh
Then, make sure you have access to Hugging Face models by setting your token:

export hf=***  # replace with your actual token
Merge the base model and LoRA adapter. If llama-factory/output/lora is missing, you can download LoRA weights from Chi UC:

https://chi.uc.chameleoncloud.org/project/containers/container/project6_model/lora

Once the LoRA files are available, run the following inside the llama-train container:

python3 /llama-factory/utils/merge.py
This will generate a merged model under llama-factory/merged_model.

Start the OpenAI-compatible inference server using vLLM:

python3 -m vllm.entrypoints.openai.api_server \
  --model ./merged_model \
  --dtype float16 \
  --port 8000 \
  --tokenizer ./merged_model
After the server is running, you can send a request via curl:

bash
Copy
Edit
curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "./merged_model",
    "prompt": "Instruction: What list in an environment?\nInput: def list_states saltenv '\''base'\'' return __context__['\''fileclient''] list_states saltenv\nOutput:",
    "max_tokens": 128,
    "temperature": 0.6
  }'