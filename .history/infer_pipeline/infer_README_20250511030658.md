# LLaMA Inference Quick Guide

To perform inference using a LoRA-finetuned LLaMA model via llama-factory and vLLM, follow the steps below.

First, clone the repository using `git clone git@github.com:Yuan-33/llama-factory.git`. Then install all dependencies by running `bash setup_infer.sh`. Before proceeding, make sure you have access to the Hugging Face model by setting your token with `export hf=***`, replacing `***` with your actual token.

Next, you need to merge the base model with your LoRA adapter weights. By default, the script searches for LoRA weights under `llama-factory/output/lora`. If they are not found, you can retrieve them from Chi UC at `https://chi.uc.chameleoncloud.org/project/containers/container/project6_model/lora`. Once the files are in place, run `python3 /llama-factory/utils/merge.py` inside the container to generate the merged model under `llama-factory/merged_model`.

To start the inference server using vLLM, run the following command: `python3 -m vllm.entrypoints.openai.api_server --model ./merged_model --dtype float16 --port 8000 --tokenizer ./merged_model`. This will expose an OpenAI-compatible API on port 8000.

Finally, you can test the endpoint using curl. Hereâ€™s an example request:  
```bash
curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "./merged_model",
    "prompt": "Instruction: What list in an environment?\nInput: def list_states saltenv '\''base'\'' return __context__['\''fileclient''] list_states saltenv\nOutput:",
    "max_tokens": 128,
    "temperature": 0.6
  }'
